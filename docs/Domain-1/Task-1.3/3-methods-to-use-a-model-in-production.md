---
sidebar_position: 3
---

# Methods to Use a Machine Learning Model in Production

Once a model is trained and evaluated, the next step is to **deploy it into production** so applications can use it to make real-time or batch predictions.

There are several methods for using a model in production:

---

## ‚òÅÔ∏è 1. Managed API Service

**What it is:**  
Cloud providers like AWS, Azure, and Google Cloud offer **fully managed services** that host your ML model and expose it as an API.

**How it works:**
- You upload the model.
- The platform handles hosting, scaling, and availability.
- You access it via a RESTful API.

**Examples:**
- **Amazon SageMaker Endpoint**
- **Google Vertex AI**
- **Azure Machine Learning Online Endpoint**

**Pros:**
- Easy to set up and maintain.
- Automatically scales to handle traffic.
- Integrated security and monitoring.

**Cons:**
- May be more expensive for high-volume traffic.
- Less control over infrastructure.

---

## üñ•Ô∏è 2. Self-Hosted API

**What it is:**  
You package your model into an app (e.g., using Flask or FastAPI in Python), then host it on your own server, cloud VM, or container platform.

**How it works:**
- Serialize the model (e.g., `.pkl`, `.pt`, `.onnx`, `.joblib`).
- Create an API server that loads the model and handles requests.
- Host it on AWS EC2, Docker, Kubernetes, etc.

**Examples:**
- Flask + Gunicorn hosted on AWS EC2
- Docker container on AWS ECS or Kubernetes

**Pros:**
- Full control over the environment.
- Can be cost-effective for stable workloads.

**Cons:**
- Requires more DevOps knowledge.
- Must handle scaling, monitoring, and security manually.

---

## ü™£ 3. Batch Prediction Pipeline

**What it is:**  
Instead of serving predictions in real time, run **scheduled jobs** that process data in bulk and store the results.

**How it works:**
- Load data (e.g., from Amazon S3).
- Run the model on the entire dataset.
- Store the output back in a database or file.

**Examples:**
- Predicting loan risk for all customers daily.
- Recommending products weekly for users.

**Pros:**
- Efficient for large datasets.
- No need for live infrastructure.

**Cons:**
- Not real-time.
- Results may be outdated for fast-changing inputs.

**Tools:**
- AWS SageMaker Batch Transform
- AWS Lambda + Step Functions
- Apache Airflow

---

## üß† 4. Using Pre-Trained Hosted Models

**What it is:**  
Directly use models hosted by cloud providers or API companies without building or training your own.

**Examples:**
- **Amazon Bedrock** for foundation models like Anthropic Claude or Titan.
- **OpenAI API** for GPT models.
- **Amazon Rekognition**, **Comprehend**, **Polly** for vision, NLP, and text-to-speech.

**Pros:**
- No need to train or deploy.
- Instant access via API.

**Cons:**
- Limited to predefined model capabilities.
- Pay per use.

---

## ‚öôÔ∏è Summary Table

| Method                    | Real-Time? | Managed? | Use Case Examples                      | Tools/Platforms                         |
| ------------------------- | ---------- | -------- | -------------------------------------- | --------------------------------------- |
| Managed API Service       | ‚úÖ Yes      | ‚úÖ Yes    | Chatbots, fraud detection              | Amazon SageMaker, Azure ML, Vertex AI   |
| Self-Hosted API           | ‚úÖ Yes      | ‚ùå No     | Custom apps, internal tools            | Flask, FastAPI, Docker, Kubernetes      |
| Batch Prediction          | ‚ùå No       | ‚úÖ/‚ùå      | Report generation, recommendation prep | SageMaker Batch, Lambda, Airflow        |
| Pre-Trained Hosted Models | ‚úÖ Yes      | ‚úÖ Yes    | Text generation, translation           | Amazon Bedrock, OpenAI API, Rekognition |

---

## ‚úÖ Choosing the Right Method

| If you...                                   | Then use...                               |
| ------------------------------------------- | ----------------------------------------- |
| Want quick and easy deployment              | Managed API Service (e.g., SageMaker)     |
| Need control over environment or cost       | Self-Hosted API (e.g., Docker + Flask)    |
| Don't need real-time results                | Batch Prediction                          |
| Just want to use AI without training models | Pre-Trained Hosted Models (e.g., Bedrock) |